{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26330c87",
   "metadata": {},
   "source": [
    "# RAG - advanced data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b89925e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/TEG_2025/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31816cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ca5c30c",
   "metadata": {},
   "source": [
    "üéØ METADATA FILTERING DEMONSTRATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7212891d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1Ô∏è‚É£ Loading documents with rich metadata:\n",
      "   Loaded 5 raw documents\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1Ô∏è‚É£ Loading documents with rich metadata:\")\n",
    "\n",
    "# Load existing scientist documents\n",
    "data_dir = \"data/scientists_bios\"\n",
    "loader = DirectoryLoader(data_dir, glob=\"*.txt\")\n",
    "raw_documents = loader.load()\n",
    "\n",
    "print(f\"   Loaded {len(raw_documents)} raw documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b9ddec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_enhanced_metadata(doc):\n",
    "    \"\"\"Extract rich metadata from document content and filename.\"\"\"\n",
    "    source_file = doc.metadata.get('source', '')\n",
    "    filename = os.path.basename(source_file).replace('.txt', '')\n",
    "    content = doc.page_content\n",
    "\n",
    "    # Extract scientist information\n",
    "    scientist_info = {\n",
    "        'scientist_name': filename,\n",
    "        'content_type': 'biography',\n",
    "        'source_type': 'text_file',\n",
    "        'language': 'english'\n",
    "    }\n",
    "\n",
    "    # Extract time periods from content\n",
    "    birth_death_info = {}\n",
    "    if '(' in content and ')' in content:\n",
    "        # Look for birth-death years in parentheses\n",
    "        import re\n",
    "        years = re.findall(r'\\((\\d{4})-(\\d{4})\\)', content)\n",
    "        if years:\n",
    "            birth_year, death_year = years[0]\n",
    "            birth_death_info.update({\n",
    "                'birth_year': int(birth_year),\n",
    "                'death_year': int(death_year),\n",
    "                'century': f\"{birth_year[:2]}th century\" if birth_year.startswith(\n",
    "                    '18') else f\"{birth_year[:2]}th century\",\n",
    "                'time_period': 'historical'\n",
    "            })\n",
    "\n",
    "    # Extract scientific fields\n",
    "    field_keywords = {\n",
    "        'mathematics': ['mathematician', 'algorithm', 'analytical', 'computation'],\n",
    "        'physics': ['physicist', 'relativity', 'Nobel Prize', 'photoelectric', 'radioactivity'],\n",
    "        'chemistry': ['chemist', 'chemical', 'elements', 'research'],\n",
    "        'computer_science': ['computer', 'programming', 'algorithm', 'machine']\n",
    "    }\n",
    "\n",
    "    fields = []\n",
    "    content_lower = content.lower()\n",
    "    for field, keywords in field_keywords.items():\n",
    "        if any(keyword in content_lower for keyword in keywords):\n",
    "            fields.append(field)\n",
    "\n",
    "    scientist_info['scientific_fields'] = fields\n",
    "    scientist_info['primary_field'] = fields[0] if fields else 'unknown'\n",
    "\n",
    "    # Add document quality metrics\n",
    "    scientist_info.update({\n",
    "        'word_count': len(content.split()),\n",
    "        'character_count': len(content),\n",
    "        'completeness': 'high' if len(content) > 200 else 'medium' if len(content) > 100 else 'low'\n",
    "    })\n",
    "\n",
    "    # Merge with existing metadata\n",
    "    doc.metadata.update(scientist_info)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'data/scientists_bios/Ada Lovelace.txt',\n",
       " 'scientist_name': 'Ada Lovelace',\n",
       " 'content_type': 'biography',\n",
       " 'source_type': 'text_file',\n",
       " 'language': 'english',\n",
       " 'scientific_fields': ['mathematics', 'computer_science'],\n",
       " 'primary_field': 'mathematics',\n",
       " 'word_count': 576,\n",
       " 'character_count': 3744,\n",
       " 'completeness': 'high'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_enhanced_metadata(raw_documents[0]).metadata"
   ],
   "id": "47e861e80b6042b0"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb9b9968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Enhanced metadata for each document:\n",
      "   üìÑ Ada Lovelace:\n",
      "      ‚Ä¢ Fields: mathematics, computer_science\n",
      "      ‚Ä¢ Primary: mathematics\n",
      "      ‚Ä¢ Birth year: unknown\n",
      "      ‚Ä¢ Word count: 576\n"
     ]
    }
   ],
   "source": [
    "# Apply enhanced metadata extraction\n",
    "enhanced_documents = []\n",
    "for doc in raw_documents:\n",
    "    enhanced_doc = extract_enhanced_metadata(doc)\n",
    "    enhanced_documents.append(enhanced_doc)\n",
    "\n",
    "print(\"\\n   Enhanced metadata for each document:\")\n",
    "for doc in enhanced_documents:\n",
    "    print(f\"   üìÑ {doc.metadata['scientist_name']}:\")\n",
    "    print(f\"      ‚Ä¢ Fields: {', '.join(doc.metadata['scientific_fields'])}\")\n",
    "    print(f\"      ‚Ä¢ Primary: {doc.metadata['primary_field']}\")\n",
    "    print(f\"      ‚Ä¢ Birth year: {doc.metadata.get('birth_year', 'unknown')}\")\n",
    "    print(f\"      ‚Ä¢ Word count: {doc.metadata['word_count']}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ee013b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff5a0204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2Ô∏è‚É£ Chunking with metadata preservation:\n",
      "   Created 28 chunks with enhanced metadata\n",
      "   Sample chunk metadata:\n",
      "      source: data/scientists_bios/Ada Lovelace.txt\n",
      "      scientist_name: Ada Lovelace\n",
      "      content_type: biography\n",
      "      source_type: text_file\n",
      "      language: english\n",
      "      scientific_fields: ['mathematics', 'computer_science']\n",
      "      primary_field: mathematics\n",
      "      word_count: 576\n",
      "      character_count: 3744\n",
      "      completeness: high\n",
      "      chunk_id: chunk_1\n",
      "      chunk_size: 756\n",
      "      chunk_position: start\n"
     ]
    }
   ],
   "source": [
    "# 2. Text Splitting with Metadata Preservation\n",
    "print(\"\\n2Ô∏è‚É£ Chunking with metadata preservation:\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(enhanced_documents)\n",
    "\n",
    "# Add chunk-specific metadata\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk.metadata.update({\n",
    "        'chunk_id': f\"chunk_{i + 1}\",\n",
    "        'chunk_size': len(chunk.page_content),\n",
    "        'chunk_position': 'start' if i < len(chunks) // 3 else 'middle' if i < 2 * len(chunks) // 3 else 'end'\n",
    "    })\n",
    "\n",
    "print(f\"   Created {len(chunks)} chunks with enhanced metadata\")\n",
    "print(\"   Sample chunk metadata:\")\n",
    "sample_chunk = chunks[0]\n",
    "for key, value in sample_chunk.metadata.items():\n",
    "    print(f\"      {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98482327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3Ô∏è‚É£ Building vector store with metadata indexing:\n",
      "   ‚úÖ Indexed 28 chunks with full metadata\n"
     ]
    }
   ],
   "source": [
    "# 3. Create Vector Store with Rich Metadata\n",
    "print(\"\\n3Ô∏è‚É£ Building vector store with metadata indexing:\")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "vector_store.add_documents(documents=chunks)\n",
    "\n",
    "print(f\"   ‚úÖ Indexed {len(chunks)} chunks with full metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f75e3edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Metadata Filtering Examples\n",
    "\n",
    "# Create filtered retrievers using custom search functions\n",
    "def search_with_field_filter(query, target_field, k=3):\n",
    "    \"\"\"Search with field filtering.\"\"\"\n",
    "    all_results = vector_store.similarity_search(query, k=k * 3)\n",
    "\n",
    "    filtered_results = []\n",
    "    for result in all_results:\n",
    "        if result.metadata.get('primary_field') == target_field:\n",
    "            filtered_results.append(result)\n",
    "        if len(filtered_results) >= k:\n",
    "            break\n",
    "\n",
    "    return filtered_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a46e1c",
   "metadata": {},
   "source": [
    "Uwaga! W momencie wywo≈Çywania similarity_search podajemy wiƒôkszƒÖ liczbƒô wynik√≥w (k*3), aby mieƒá pewno≈õƒá, ≈ºe po filtrowaniu zostanie wystarczajƒÖca liczba dokument√≥w do zwr√≥cenia.\n",
    "\n",
    "Robimy to pozniewa≈º filtrowanie nastƒôpuje po wyszukiwaniu wektorowym, wiƒôc nie mamy gwarancji, ≈ºe pierwsze k wynik√≥w spe≈Çni warunki filtrowania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdade4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4Ô∏è‚É£ Metadata filtering demonstrations:\n",
      "\n",
      "   üî¨ Field-specific retrieval (Physics only):\n",
      "   Query: What are the major scientific contributions?\n",
      "   Results: 3 physics-related chunks\n",
      "   1. Albert Einstein (physics):\n",
      "\n",
      "# Albert Einstein (1879\n",
      "\n",
      "1955)\n",
      "\n",
      "Albert Einstein was a German-born theoretical physicist who develope...\n",
      "\n",
      "   2. Albert Einstein (physics):\n",
      "\n",
      "Einstein died on April 18, 1955, in Princeton, New Jersey. His brain was preserved for scientific st...\n",
      "\n",
      "   3. Albert Einstein (physics):\n",
      "\n",
      "**Photoelectric Effect\n",
      "\n",
      "**: Explained the photoelectric effect in 1905, proposing that light consist...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n4Ô∏è‚É£ Metadata filtering demonstrations:\")\n",
    "\n",
    "# Example 1: Field-specific retrieval\n",
    "print(\"\\n   üî¨ Field-specific retrieval (Physics only):\")\n",
    "\n",
    "physics_query = \"What are the major scientific contributions?\"\n",
    "physics_results = search_with_field_filter(physics_query, \"physics\")\n",
    "print(f\"   Query: {physics_query}\")\n",
    "print(f\"   Results: {len(physics_results)} physics-related chunks\")\n",
    "for i, result in enumerate(physics_results):\n",
    "    scientist = result.metadata['scientist_name']\n",
    "    field = result.metadata['primary_field']\n",
    "    print(f\"   {i + 1}. {scientist} ({field}):\")\n",
    "    print()\n",
    "    print(f\"{result.page_content[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2803b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5Ô∏è‚É£ Building contextual RAG system:\n"
     ]
    }
   ],
   "source": [
    "# 5. Contextual RAG with Metadata\n",
    "print(\"\\n5Ô∏è‚É£ Building contextual RAG system:\")\n",
    "\n",
    "llm = AzureChatOpenAI(model=\"gpt-5-nano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cd13807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_with_time_filter(query, target_century=\"19th\", k=3):\n",
    "    \"\"\"Search with time period filtering.\"\"\"\n",
    "    # Get all results first\n",
    "    all_results = vector_store.similarity_search(query, k=k * 3)\n",
    "\n",
    "    # Filter by time period\n",
    "    filtered_results = []\n",
    "    for result in all_results:\n",
    "        birth_year = result.metadata.get('birth_year')\n",
    "        if birth_year:\n",
    "            if target_century == \"19th\" and 1800 <= birth_year < 1900:\n",
    "                filtered_results.append(result)\n",
    "        if len(filtered_results) >= k:\n",
    "            break\n",
    "\n",
    "    return filtered_results\n",
    "\n",
    "\n",
    "def search_with_field_filter(query, target_field, k=3):\n",
    "    \"\"\"Search with field filtering.\"\"\"\n",
    "    all_results = vector_store.similarity_search(query, k=k * 3)\n",
    "\n",
    "    filtered_results = []\n",
    "    for result in all_results:\n",
    "        if result.metadata.get('primary_field') == target_field:\n",
    "            filtered_results.append(result)\n",
    "        if len(filtered_results) >= k:\n",
    "            break\n",
    "\n",
    "    return filtered_results\n",
    "\n",
    "\n",
    "def search_with_quality_filter(query, min_completeness=\"high\", k=3):\n",
    "    \"\"\"Search with document quality filtering.\"\"\"\n",
    "    all_results = vector_store.similarity_search(query, k=k * 2)\n",
    "\n",
    "    filtered_results = []\n",
    "    for result in all_results:\n",
    "        if result.metadata.get('completeness') == min_completeness:\n",
    "            filtered_results.append(result)\n",
    "        if len(filtered_results) >= k:\n",
    "            break\n",
    "\n",
    "    return filtered_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188063bb",
   "metadata": {},
   "source": [
    "poni≈ºej przygotujemy bardzo prstry retriever, kt√≥ry szuka s≈Ç√≥w-kluczy w metadanych i zwraca tylko pasujƒÖce dokumenty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d504775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smart retriever that uses context to determine filters\n",
    "def create_contextual_retriever(query, k=4):\n",
    "    \"\"\"Create a context-aware retriever based on query content.\"\"\"\n",
    "    query_lower = query.lower()\n",
    "\n",
    "    # Determine appropriate filters based on query\n",
    "    filters = {}\n",
    "\n",
    "    # Field-specific keywords\n",
    "    if any(word in query_lower for word in ['physics', 'relativity', 'einstein']):\n",
    "        filters['primary_field'] = 'physics'\n",
    "    elif any(word in query_lower for word in ['mathematics', 'algorithm', 'computation']):\n",
    "        filters['primary_field'] = 'mathematics'\n",
    "    elif any(word in query_lower for word in ['programming', 'computer', 'lovelace']):\n",
    "        filters['primary_field'] = 'computer_science'\n",
    "\n",
    "    # Time-based keywords\n",
    "    historical_terms = ['historical', 'past', 'old', '19th century', 'early']\n",
    "    if any(term in query_lower for term in historical_terms):\n",
    "        # Use custom search for historical filtering\n",
    "        return search_with_time_filter(query, \"19th\", k)\n",
    "\n",
    "    # Quality-based keywords\n",
    "    if any(word in query_lower for word in ['detailed', 'comprehensive', 'complete']):\n",
    "        return search_with_quality_filter(query, \"high\", k)\n",
    "\n",
    "    # Use filters if determined\n",
    "    if 'primary_field' in filters:\n",
    "        return search_with_field_filter(query, filters['primary_field'], k)\n",
    "    else:\n",
    "        return vector_store.similarity_search(query, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced prompt that uses metadata\n",
    "contextual_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an assistant for question-answering tasks about scientists and their contributions.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "Pay attention to the metadata information about each source, including:\n",
    "- The scientist's name and primary field\n",
    "- Time period and historical context\n",
    "- Document quality and completeness\n",
    "\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context with metadata:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "\"\"\")"
   ],
   "id": "dace8c9b059d4c65"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b34446da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context_with_metadata(retrieved_docs):\n",
    "    \"\"\"Format retrieved documents with their metadata for the prompt.\"\"\"\n",
    "    formatted_context = []\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        metadata = doc.metadata\n",
    "        scientist = metadata.get('scientist_name', 'Unknown')\n",
    "        field = metadata.get('primary_field', 'Unknown')\n",
    "        birth_year = metadata.get('birth_year', 'Unknown')\n",
    "\n",
    "        context_entry = f\"\"\"\n",
    "Source {i + 1}: {scientist} ({field}, born {birth_year})\n",
    "Content: {doc.page_content}\n",
    "\"\"\"\n",
    "        formatted_context.append(context_entry)\n",
    "\n",
    "    return \"\\n\".join(formatted_context)\n",
    "\n",
    "\n",
    "# Create contextual RAG chain\n",
    "def contextual_rag_chain(question):\n",
    "    \"\"\"RAG chain with contextual retrieval and metadata-aware prompting.\"\"\"\n",
    "    # Get contextually relevant documents\n",
    "    retrieved_docs = create_contextual_retriever(question)\n",
    "\n",
    "    # Format context with metadata\n",
    "    formatted_context = format_context_with_metadata(retrieved_docs)\n",
    "\n",
    "    # Generate response\n",
    "    response = llm.invoke(\n",
    "        contextual_prompt.format(\n",
    "            question=question,\n",
    "            context=formatted_context\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return response.content, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6Ô∏è‚É£ Testing contextual RAG system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A2: Albert Einstein developed the special theory of relativity (1905) and the general theory of relativity (1915), reshaping physics of space, time, and gravity. He formulated the mass‚Äìenergy equivalence E=mc¬≤. He explained the photoelectric effect (1905), showing light as photons and helping establish quantum theory, and provided evidence for Brownian motion supporting atomic theory.\n",
      "\n",
      "   üìö Sources used (4 documents):\n",
      "      1. Albert Einstein (physics)\n",
      "      2. Albert Einstein (physics)\n",
      "      3. Albert Einstein (physics)\n",
      "      4. Albert Einstein (physics)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n6Ô∏è‚É£ Testing contextual RAG system:\")\n",
    "\n",
    "question = \"What physics discoveries were made by Einstein?\"\n",
    "\n",
    "try:\n",
    "    answer, sources = contextual_rag_chain(question)\n",
    "    print(f\"   A{i}: {answer}\")\n",
    "\n",
    "    print(f\"\\n   üìö Sources used ({len(sources)} documents):\")\n",
    "    for j, source in enumerate(sources):\n",
    "        scientist = source.metadata['scientist_name']\n",
    "        field = source.metadata['primary_field']\n",
    "        print(f\"      {j + 1}. {scientist} ({field})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   A{i}: Error - {str(e)}\")"
   ],
   "id": "4938fac0cc68a6ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ddd348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7731ba3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb190b8c",
   "metadata": {},
   "source": [
    "üîÄ HYBRID SEARCH DEMONSTRATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1Ô∏è‚É£ Loading documents for hybrid search:\n",
      "   Loaded 5 documents\n",
      "   Created 28 chunks for hybrid search\n",
      "\n",
      "2Ô∏è‚É£ Building multiple search indexes:\n",
      "   ‚úÖ Vector store: 28 chunks embedded\n"
     ]
    }
   ],
   "source": [
    "# 1. Load and Prepare Documents\n",
    "print(\"\\n1Ô∏è‚É£ Loading documents for hybrid search:\")\n",
    "\n",
    "data_dir = \"data/scientists_bios\"\n",
    "loader = DirectoryLoader(data_dir, glob=\"*.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"   Loaded {len(documents)} documents\")\n",
    "\n",
    "# Add metadata\n",
    "for doc in documents:\n",
    "    filename = os.path.basename(doc.metadata['source']).replace('.txt', '')\n",
    "    doc.metadata.update({\n",
    "        'scientist_name': filename,\n",
    "        'word_count': len(doc.page_content.split())\n",
    "    })\n",
    "\n",
    "# Chunk documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"   Created {len(chunks)} chunks for hybrid search\")\n",
    "\n",
    "# 2. Build Multiple Search Indexes\n",
    "print(\"\\n2Ô∏è‚É£ Building multiple search indexes:\")\n",
    "\n",
    "# Vector search setup\n",
    "embeddings = AzureOpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "vector_store.add_documents(documents=chunks)\n",
    "print(f\"   ‚úÖ Vector store: {len(chunks)} chunks embedded\")"
   ],
   "id": "8a7204d724ba9b4e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c64e1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51efaa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09ab980e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ BM25 index: 28 documents indexed\n"
     ]
    }
   ],
   "source": [
    "# BM25 keyword search setup\n",
    "chunk_texts = [chunk.page_content for chunk in chunks]\n",
    "tokenized_chunks = [text.lower().split() for text in chunk_texts]\n",
    "bm25 = BM25Okapi(tokenized_chunks)\n",
    "print(f\"   ‚úÖ BM25 index: {len(chunk_texts)} documents indexed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1a15ec",
   "metadata": {},
   "source": [
    "BM25 to jedna z najpopularniejszych funkcji rankingowych stosowanych w wyszukiwarkach, kt√≥ra ocenia, jak bardzo dany dokument jest trafny wzglƒôdem zapytania u≈ºytkownika. Model ten opiera siƒô na probabilistycznym podej≈õciu do wyszukiwania informacji.\n",
    "\n",
    "$$\n",
    "{\\displaystyle {\\text{score}}(D,Q)=\\sum _{i=1}^{n}{\\text{IDF}}(q_{i})\\cdot {\\frac {f(q_{i},D)\\cdot (k_{1}+1)}{f(q_{i},D)+k_{1}\\cdot \\left(1-b+b\\cdot {\\frac {|D|}{\\text{avgdl}}}\\right)}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\\displaystyle {\\text{IDF}}(q_{i})=\\ln \\left({\\frac {N-n(q_{i})+0.5}{n(q_{i})+0.5}}+1\\right)}\n",
    "$$\n",
    "\n",
    "IDF - Inverse Document Frequency\n",
    "\n",
    "https://en.wikipedia.org/wiki/Okapi_BM25\n",
    "\n",
    "TF-IDF - iloczyn Term frequency oraz Inverse Document Frequency\n",
    "\n",
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00be93ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ TF-IDF index: 1000 features extracted\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF setup for additional keyword matching\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=1000,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(chunk_texts)\n",
    "print(f\"   ‚úÖ TF-IDF index: {tfidf_matrix.shape[1]} features extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   üîç Test query: What did Einstein discover about light and energy?\n"
     ]
    }
   ],
   "source": [
    "# Test each method\n",
    "test_query = \"What did Einstein discover about light and energy?\"\n",
    "print(f\"\\n   üîç Test query: {test_query}\")"
   ],
   "id": "991be4b4b956fd71"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90654b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   üß† Vector search results:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1. Albert Einstein (score: 0.582): **Photoelectric Effect\n",
      "\n",
      "**: Explained the photoelectric effect in 1905, proposin...\n",
      "      2. Albert Einstein (score: 0.557): # Albert Einstein (1879\n",
      "\n",
      "1955)\n",
      "\n",
      "Albert Einstein was a German-born theoretical ph...\n",
      "      3. Albert Einstein (score: 0.459): Einstein died on April 18, 1955, in Princeton, New Jersey. His brain was preserv...\n"
     ]
    }
   ],
   "source": [
    "def vector_search(query, k=5):\n",
    "    \"\"\"Semantic similarity search using embeddings.\"\"\"\n",
    "    results = vector_store.similarity_search_with_score(query, k=k)\n",
    "    return [(doc, score) for doc, score in results]\n",
    "\n",
    "\n",
    "print(f\"\\n   üß† Vector search results:\")\n",
    "vector_results = vector_search(test_query, k=3)\n",
    "for i, (doc, score) in enumerate(vector_results):\n",
    "    scientist = doc.metadata['scientist_name']\n",
    "    preview = doc.page_content[:80] + \"...\"\n",
    "    print(f\"      {i + 1}. {scientist} (score: {score:.3f}): {preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16ece4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   üî§ BM25 search results:\n",
      "      1. Ada Lovelace (score: 5.453): Lovelace's notes also included visionary insights about the potential of computi...\n",
      "      2. Albert Einstein (score: 5.296): Einstein married twice, first to Mileva Mariƒá, with whom he had three children, ...\n",
      "      3. Albert Einstein (score: 4.813): **Photoelectric Effect\n",
      "\n",
      "**: Explained the photoelectric effect in 1905, proposin...\n"
     ]
    }
   ],
   "source": [
    "def bm25_search(query, k=5):\n",
    "    \"\"\"Keyword search using BM25.\"\"\"\n",
    "    query_tokens = query.lower().split()\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "\n",
    "    # Get top-k results\n",
    "    top_indices = np.argsort(scores)[::-1][:k]\n",
    "    results = []\n",
    "\n",
    "    for idx in top_indices:\n",
    "        if scores[idx] > 0:  # Only include non-zero scores\n",
    "            results.append((chunks[idx], scores[idx]))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "print(f\"\\n   üî§ BM25 search results:\")\n",
    "bm25_results = bm25_search(test_query, k=3)\n",
    "for i, (doc, score) in enumerate(bm25_results):\n",
    "    scientist = doc.metadata['scientist_name']\n",
    "    preview = doc.page_content[:80] + \"...\"\n",
    "    print(f\"      {i + 1}. {scientist} (score: {score:.3f}): {preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1465231d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   üìä TF-IDF search results:\n",
      "      1. Albert Einstein (score: 0.239): # Albert Einstein (1879\n",
      "\n",
      "1955)\n",
      "\n",
      "Albert Einstein was a German-born theoretical ph...\n",
      "      2. Albert Einstein (score: 0.146): Einstein died on April 18, 1955, in Princeton, New Jersey. His brain was preserv...\n",
      "      3. Albert Einstein (score: 0.125): Einstein married twice, first to Mileva Mariƒá, with whom he had three children, ...\n"
     ]
    }
   ],
   "source": [
    "def tfidf_search(query, k=5):\n",
    "    \"\"\"TF-IDF based keyword search.\"\"\"\n",
    "    query_vec = tfidf_vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "\n",
    "    top_indices = np.argsort(similarities)[::-1][:k]\n",
    "    results = []\n",
    "\n",
    "    for idx in top_indices:\n",
    "        if similarities[idx] > 0:\n",
    "            results.append((chunks[idx], similarities[idx]))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "print(f\"\\n   üìä TF-IDF search results:\")\n",
    "tfidf_results = tfidf_search(test_query, k=3)\n",
    "for i, (doc, score) in enumerate(tfidf_results):\n",
    "    scientist = doc.metadata['scientist_name']\n",
    "    preview = doc.page_content[:80] + \"...\"\n",
    "    print(f\"      {i + 1}. {scientist} (score: {score:.3f}): {preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc29ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "528a32aa",
   "metadata": {},
   "source": [
    "Score fusion - ≈ÇƒÖczenie wynik√≥w (ranking√≥w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5Ô∏è‚É£ Score fusion strategies:\n"
     ]
    }
   ],
   "source": [
    "# 5. Score Fusion Strategies\n",
    "print(\"\\n5Ô∏è‚É£ Score fusion strategies:\")\n",
    "\n",
    "\n",
    "def normalize_scores(scores, method='min_max'):\n",
    "    \"\"\"Normalize scores to 0-1 range.\"\"\"\n",
    "    scores = np.array(scores)\n",
    "    if method == 'min_max':\n",
    "        min_score, max_score = scores.min(), scores.max()\n",
    "        if max_score > min_score:\n",
    "            return (scores - min_score) / (max_score - min_score)\n",
    "    elif method == 'z_score':\n",
    "        mean, std = scores.mean(), scores.std()\n",
    "        if std > 0:\n",
    "            return (scores - mean) / std\n",
    "    return scores\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(results_list, k=60):\n",
    "    \"\"\"Combine rankings using Reciprocal Rank Fusion.\"\"\"\n",
    "    doc_scores = {}\n",
    "\n",
    "    for results in results_list:\n",
    "        for rank, (doc, _) in enumerate(results):\n",
    "            doc_id = id(doc)  # Use object id as unique identifier\n",
    "            if doc_id not in doc_scores:\n",
    "                doc_scores[doc_id] = {'doc': doc, 'score': 0}\n",
    "            doc_scores[doc_id]['score'] += 1 / (k + rank + 1)\n",
    "\n",
    "    # Sort by combined score\n",
    "    sorted_results = sorted(doc_scores.values(), key=lambda x: x['score'], reverse=True)\n",
    "    return [(item['doc'], item['score']) for item in sorted_results]\n",
    "\n",
    "\n",
    "def weighted_score_fusion(vector_results, keyword_results, vector_weight=0.6):\n",
    "    \"\"\"Combine results using weighted score fusion.\"\"\"\n",
    "    # Normalize scores\n",
    "    vector_scores = [score for _, score in vector_results]\n",
    "    keyword_scores = [score for _, score in keyword_results]\n",
    "\n",
    "    norm_vector_scores = normalize_scores(vector_scores)\n",
    "    norm_keyword_scores = normalize_scores(keyword_scores)\n",
    "\n",
    "    # Create combined results\n",
    "    doc_scores = {}\n",
    "\n",
    "    # Add vector results\n",
    "    for i, (doc, _) in enumerate(vector_results):\n",
    "        doc_id = id(doc)\n",
    "        doc_scores[doc_id] = {\n",
    "            'doc': doc,\n",
    "            'vector_score': norm_vector_scores[i],\n",
    "            'keyword_score': 0\n",
    "        }\n",
    "\n",
    "    # Add keyword results\n",
    "    for i, (doc, _) in enumerate(keyword_results):\n",
    "        doc_id = id(doc)\n",
    "        if doc_id in doc_scores:\n",
    "            doc_scores[doc_id]['keyword_score'] = norm_keyword_scores[i]\n",
    "        else:\n",
    "            doc_scores[doc_id] = {\n",
    "                'doc': doc,\n",
    "                'vector_score': 0,\n",
    "                'keyword_score': norm_keyword_scores[i]\n",
    "            }\n",
    "\n",
    "    # Calculate combined scores\n",
    "    for doc_id in doc_scores:\n",
    "        doc_scores[doc_id]['combined_score'] = (\n",
    "                vector_weight * doc_scores[doc_id]['vector_score'] +\n",
    "                (1 - vector_weight) * doc_scores[doc_id]['keyword_score']\n",
    "        )\n",
    "\n",
    "    # Sort by combined score\n",
    "    sorted_results = sorted(doc_scores.values(), key=lambda x: x['combined_score'], reverse=True)\n",
    "    return [(item['doc'], item['combined_score']) for item in sorted_results]\n"
   ],
   "id": "d75951466db58e12"
  },
  {
   "cell_type": "markdown",
   "id": "0fea50c4",
   "metadata": {},
   "source": [
    "Reciprocal Rank Fusion\n",
    "\n",
    "$$\n",
    "RRF = \\sum{ \\frac{1}{{k + rank + 1}} }\n",
    "$$\n",
    "\n",
    "Reciprocal Rank Fusion\n",
    "\n",
    "$$\n",
    "WSF = w * score_1 + (1-w) * score_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   üîç Fusion test query: Einstein's theory of relativity and light\n"
     ]
    }
   ],
   "source": [
    "# Test fusion strategies\n",
    "fusion_query = \"Einstein's theory of relativity and light\"\n",
    "print(f\"\\n   üîç Fusion test query: {fusion_query}\")\n",
    "\n",
    "vector_results = vector_search(fusion_query, k=5)\n",
    "bm25_results = bm25_search(fusion_query, k=5)"
   ],
   "id": "c08db49534966da2"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5b0fa94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   üîó Reciprocal Rank Fusion:\n",
      "      1. Albert Einstein (RRF: 0.016): # Albert Einstein (1879\n",
      "\n",
      "1955)\n",
      "\n",
      "Albert Einstein was a German...\n",
      "      2. Albert Einstein (RRF: 0.016): # Albert Einstein (1879\n",
      "\n",
      "1955)\n",
      "\n",
      "Albert Einstein was a German...\n",
      "      3. Albert Einstein (RRF: 0.016): **Photoelectric Effect\n",
      "\n",
      "**: Explained the photoelectric effe...\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n   üîó Reciprocal Rank Fusion:\")\n",
    "rrf_results = reciprocal_rank_fusion([vector_results, bm25_results])\n",
    "for i, (doc, score) in enumerate(rrf_results[:3]):\n",
    "    scientist = doc.metadata['scientist_name']\n",
    "    preview = doc.page_content[:60] + \"...\"\n",
    "    print(f\"      {i + 1}. {scientist} (RRF: {score:.3f}): {preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8b45601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ‚öñÔ∏è Weighted Score Fusion (60% vector, 40% keyword):\n",
      "      1. Albert Einstein (WSF: 0.600): # Albert Einstein (1879\n",
      "\n",
      "1955)\n",
      "\n",
      "Albert Einstein was a German...\n",
      "      2. Albert Einstein (WSF: 0.400): # Albert Einstein (1879\n",
      "\n",
      "1955)\n",
      "\n",
      "Albert Einstein was a German...\n",
      "      3. Albert Einstein (WSF: 0.397): **Photoelectric Effect\n",
      "\n",
      "**: Explained the photoelectric effe...\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n   ‚öñÔ∏è Weighted Score Fusion (60% vector, 40% keyword):\")\n",
    "wsf_results = weighted_score_fusion(vector_results, bm25_results, vector_weight=0.6)\n",
    "for i, (doc, score) in enumerate(wsf_results[:3]):\n",
    "    scientist = doc.metadata['scientist_name']\n",
    "    preview = doc.page_content[:60] + \"...\"\n",
    "    print(f\"      {i + 1}. {scientist} (WSF: {score:.3f}): {preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15ffffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca18ca89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b159b086",
   "metadata": {},
   "source": [
    "Query expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c65c7",
   "metadata": {},
   "source": [
    "Query expansion (rozszerzanie zapyta≈Ñ) to technika, kt√≥ra polega na automatycznym \"wzbogaceniu\" zapytania u≈ºytkownika o dodatkowe s≈Çowa kluczowe, synonimy, terminy techniczne lub kontekstowe informacje, aby system wyszukiwania m√≥g≈Ç znale≈∫ƒá szerszy i bardziej trafny zestaw dokument√≥w. Problem, kt√≥ry rozwiƒÖzuje, jest prosty: ludzie czƒôsto wpisujƒÖ zapytania kr√≥tkie i nieprecyzyjne (\"Newton‚Äôs work\"), podczas gdy dokumenty w bazie u≈ºywajƒÖ zupe≈Çnie innych okre≈õle≈Ñ (\"laws of motion\", \"gravity research\").\n",
    "\n",
    "Istnieje kilka klas strategii query expansion:\n",
    "\n",
    "Proste metody leksykalne, takie jak synonimy, odmiany s≈Ç√≥w i s≈Çowa bliskoznaczne, poprawiajƒÖ podstawowe dopasowanie s≈Ç√≥w kluczowych. \n",
    "\n",
    "Metody konceptualne, oparte na mapie pojƒôƒá lub ontologii, dodajƒÖ terminy logicznie zwiƒÖzane z tematem (np. \"Einstein ‚Üí relativity, spacetime\"). \n",
    "\n",
    "LLM-based query expansion generuje nowe zapytania lub ich warianty, w tym techniczne, opisowe lub alternatywne, dziƒôki czemu lepiej pokrywa r√≥≈ºne sposoby opisu tego samego zjawiska. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52728d0b",
   "metadata": {},
   "source": [
    "Zadanie: \n",
    "\n",
    "1. Wiƒôcej przyk≈Çad√≥w w `1_metadata_filtering.py`,`2_hybrid_search.py`\n",
    "2. Plik `3_query_expansion.py`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c555dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d84fc04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84086c7f",
   "metadata": {},
   "source": [
    "Post-processing: re-ranking\n",
    "\n",
    "Re-ranking to drugi etap wyszukiwania, kt√≥ry bierze dokumenty znalezione w pierwszym kroku (np. przez wektory lub BM25) i ponownie je sortuje wed≈Çug trafno≈õci - ale dok≈Çadniejszym, inteligentniejszym modelem.\n",
    "\n",
    "üí° Po co robi siƒô re-ranking?\n",
    "Bo same wektory nie sƒÖ idealne.\n",
    "\n",
    "Embeddingi patrzƒÖ tylko na odleg≈Ço≈õƒá wektor√≥w, bez rozumienia sk≈Çadni.\n",
    "\n",
    "BM25 patrzy tylko na s≈Çowa, nie na semantykƒô.\n",
    "\n",
    "Cross-encoder czy LLM czyta ca≈Çe zdanie/dokument + pytanie i ocenia trafno≈õƒá.\n",
    "\n",
    "Dlatego re-ranking daje najwy≈ºszƒÖ jako≈õƒá retrievalu w RAG.\n",
    "\n",
    "üß† Czym jest cross-encoder?\n",
    "\n",
    "Cross-encoder to rodzaj sieci neuronowej, kt√≥ry bierze dwa teksty naraz - np. pytanie + dokument - i przetwarza je wsp√≥lnie, w jednym ciƒÖgu token√≥w, ≈ºeby oceniƒá, jak bardzo do siebie pasujƒÖ.\n",
    "\n",
    "Przyk≈Çad pary wej≈õciowej:\n",
    "\n",
    "[CLS] What did Einstein discover? [SEP] Einstein developed the theory of relativity... [SEP]\n",
    "\n",
    "Model czyta to jako jeden po≈ÇƒÖczony tekst i oblicza pojedynczy wynik (score), np. 0.91 - im wy≈ºej, tym bardziej dokument odpowiada zapytaniu.\n",
    "\n",
    "To naprawdƒô jest sieƒá neuronowa, najczƒô≈õciej Transformer (np. BERT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1Ô∏è‚É£ Loading documents for re-ranking:\n",
      "   Loaded 5 documents, created 28 chunks\n",
      "   ‚úÖ Vector store ready with 28 indexed chunks\n"
     ]
    }
   ],
   "source": [
    "# 1. Load and Prepare Documents\n",
    "print(\"\\n1Ô∏è‚É£ Loading documents for re-ranking:\")\n",
    "\n",
    "data_dir = \"data/scientists_bios\"\n",
    "loader = DirectoryLoader(data_dir, glob=\"*.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Add metadata\n",
    "for doc in documents:\n",
    "    filename = os.path.basename(doc.metadata['source']).replace('.txt', '')\n",
    "    doc.metadata['scientist_name'] = filename\n",
    "\n",
    "# Chunk documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"   Loaded {len(documents)} documents, created {len(chunks)} chunks\")\n",
    "\n",
    "# Build vector store\n",
    "embeddings = AzureOpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "vector_store.add_documents(documents=chunks)\n",
    "\n",
    "print(f\"   ‚úÖ Vector store ready with {len(chunks)} indexed chunks\")"
   ],
   "id": "1a151becbd3ac1d6"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fdf49a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2Ô∏è‚É£ Loading cross-encoder models for re-ranking:\n",
      "   ‚úÖ MS-MARCO MiniLM cross-encoder loaded\n",
      "   ‚úÖ QNLI Electra cross-encoder loaded\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# 2. Load Cross-Encoder Models\n",
    "print(\"\\n2Ô∏è‚É£ Loading cross-encoder models for re-ranking:\")\n",
    "\n",
    "# Load different cross-encoder models for comparison\n",
    "cross_encoders = {}\n",
    "\n",
    "try:\n",
    "    # Lightweight cross-encoder for general ranking\n",
    "    cross_encoders['ms-marco'] = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "    print(\"   ‚úÖ MS-MARCO MiniLM cross-encoder loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Failed to load MS-MARCO model: {e}\")\n",
    "\n",
    "try:\n",
    "    # More specific cross-encoder for question-answering\n",
    "    cross_encoders['qnli'] = CrossEncoder('cross-encoder/qnli-electra-base')\n",
    "    print(\"   ‚úÖ QNLI Electra cross-encoder loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Failed to load QNLI model: {e}\")\n",
    "\n",
    "if not cross_encoders:\n",
    "    print(\"   ‚ö†Ô∏è No cross-encoders loaded, using fallback scoring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77b6d252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4Ô∏è‚É£ Testing individual re-ranking methods:\n",
      "\n",
      "   üîç Test query: What did Einstein discover about the universe?\n",
      "\n",
      "   üìä Initial vector search results:\n",
      "      1. Albert Einstein (score: 0.513): # Albert Einstein (1879\\n\\n1955)...\n",
      "      2. Albert Einstein (score: 0.493): Einstein died on April 18, 195...\n",
      "      3. Albert Einstein (score: 0.465): **Photoelectric Effect\\n\\n**: Ex...\n",
      "      4. Isaac Newton (score: 0.464): **Universal Gravitation\\n\\n**: N...\n",
      "      5. Albert Einstein (score: 0.404): Einstein married twice, first ...\n",
      "      6. Isaac Newton (score: 0.355): Known for his difficult person...\n"
     ]
    }
   ],
   "source": [
    "# Test Individual Re-ranking Methods\n",
    "print(\"\\n4Ô∏è‚É£ Testing individual re-ranking methods:\")\n",
    "\n",
    "# Get initial results for testing\n",
    "test_query = \"What did Einstein discover about the universe?\"\n",
    "initial_results = vector_store.similarity_search_with_score(test_query, k=6)\n",
    "\n",
    "print(f\"\\n   üîç Test query: {test_query}\")\n",
    "print(f\"\\n   üìä Initial vector search results:\")\n",
    "for i, (doc, score) in enumerate(initial_results):\n",
    "    scientist = doc.metadata['scientist_name']\n",
    "    preview = doc.page_content[:30].replace(\"\\n\", \"\\\\n\") + \"...\"\n",
    "    print(f\"      {i + 1}. {scientist} (score: {score:.3f}): {preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da6059c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1ffaa24",
   "metadata": {},
   "source": [
    "Cross-encoder model ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08cebe03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3Ô∏è‚É£ Implementing re-ranking functions:\n"
     ]
    }
   ],
   "source": [
    "# Basic Re-ranking Functions\n",
    "print(\"\\n3Ô∏è‚É£ Implementing re-ranking functions:\")\n",
    "\n",
    "\n",
    "def cross_encoder_rerank(query, documents, model_name='ms-marco', top_k=None):\n",
    "    \"\"\"Re-rank documents using cross-encoder models.\"\"\"\n",
    "    if model_name not in cross_encoders:\n",
    "        print(f\"   ‚ö†Ô∏è Model {model_name} not available, returning original order\")\n",
    "        return documents\n",
    "\n",
    "    model = cross_encoders[model_name]\n",
    "\n",
    "    # Prepare query-document pairs\n",
    "    query_doc_pairs = [(query, doc.page_content) for doc in documents]\n",
    "\n",
    "    # Get relevance scores\n",
    "    scores = model.predict(query_doc_pairs)\n",
    "\n",
    "    # Sort documents by scores\n",
    "    doc_score_pairs = list(zip(documents, scores))\n",
    "    doc_score_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return top_k or all documents\n",
    "    if top_k:\n",
    "        return doc_score_pairs[:top_k]\n",
    "    else:\n",
    "        return doc_score_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74b669fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   üéØ Cross-encoder re-ranking:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   MS-MARCO re-ranking:\n",
      "      1. Albert Einstein (score: 3.893): # Albert Einstein (1879\\n\\n1955)...\n",
      "         (original rank: 0)\n",
      "      2. Albert Einstein (score: 1.770): Einstein died on April 18, 195...\n",
      "         (original rank: 1)\n",
      "      3. Albert Einstein (score: -1.362): **Photoelectric Effect\\n\\n**: Ex...\n",
      "         (original rank: 2)\n",
      "      4. Isaac Newton (score: -2.035): **Universal Gravitation\\n\\n**: N...\n",
      "         (original rank: 3)\n",
      "\n",
      "   QNLI re-ranking:\n",
      "      1. Albert Einstein (score: 0.159): # Albert Einstein (1879\\n\\n1955)...\n",
      "         (original rank: 0)\n",
      "      2. Albert Einstein (score: 0.003): **Photoelectric Effect\\n\\n**: Ex...\n",
      "         (original rank: 2)\n",
      "      3. Isaac Newton (score: 0.002): **Universal Gravitation\\n\\n**: N...\n",
      "         (original rank: 3)\n",
      "      4. Albert Einstein (score: 0.001): Einstein married twice, first ...\n",
      "         (original rank: 4)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n   üéØ Cross-encoder re-ranking:\")\n",
    "documents_only = [doc for doc, score in initial_results]\n",
    "\n",
    "for model_name in cross_encoders:\n",
    "    reranked = cross_encoder_rerank(test_query, documents_only, model_name, top_k=4)\n",
    "    print(f\"\\n   {model_name.upper()} re-ranking:\")\n",
    "    for i, (doc, score) in enumerate(reranked):\n",
    "        scientist = doc.metadata['scientist_name']\n",
    "        preview = doc.page_content[:30].replace(\"\\n\", \"\\\\n\") + \"...\"\n",
    "        print(f\"      {i + 1}. {scientist} (score: {score:.3f}): {preview}\")\n",
    "        # find original id\n",
    "        for idx, (orig_doc, orig_score) in enumerate(initial_results):\n",
    "            if orig_doc == doc:\n",
    "                original_id = idx\n",
    "                break\n",
    "        print(f\"         (original rank: {original_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da45b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59e6b315",
   "metadata": {},
   "source": [
    "LLM-as-a-judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa15a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class LLMResponse(BaseModel):\n",
    "    score: int = Field(..., description=\"Relevance score from 1 to 10\")\n",
    "    explanation: str = Field(..., description=\"Brief explanation for the score\")\n",
    "\n",
    "\n",
    "def llm_relevance_scoring(query, documents, llm):\n",
    "    \"\"\"Use LLM to score document relevance.\"\"\"\n",
    "    relevance_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Rate the relevance of the following document to the query on a scale of 1-10.\n",
    "Consider how well the document answers the question or provides relevant information.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Document: {document}\n",
    "\n",
    "Provide only a numeric score (1-10) with brief explanation\n",
    "\"\"\")\n",
    "\n",
    "    scored_documents = []\n",
    "\n",
    "    for doc in documents:\n",
    "        try:\n",
    "            llm_with_structured_output = llm.with_structured_output(LLMResponse)\n",
    "\n",
    "            response = llm_with_structured_output.invoke(\n",
    "                relevance_prompt.format(\n",
    "                    query=query,\n",
    "                    document=doc.page_content[:500]  # Limit content for efficiency\n",
    "                )\n",
    "            )\n",
    "\n",
    "            scored_documents.append((doc, response.score))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è LLM scoring failed for document: {e}\")\n",
    "            scored_documents.append((doc, 5.0))  # Default score\n",
    "\n",
    "    # Sort by score\n",
    "    scored_documents.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scored_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28d744a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ü§ñ LLM relevance scoring:\n",
      "      1. Albert Einstein (score: 7.0): # Albert Einstein (1879\\n\\n1955)...\n",
      "         (original rank: 0)\n",
      "      2. Albert Einstein (score: 3.0): Einstein died on April 18, 195...\n",
      "         (original rank: 1)\n",
      "      3. Albert Einstein (score: 3.0): **Photoelectric Effect\\n\\n**: Ex...\n",
      "         (original rank: 2)\n",
      "      4. Isaac Newton (score: 1.0): **Universal Gravitation\\n\\n**: N...\n",
      "         (original rank: 3)\n"
     ]
    }
   ],
   "source": [
    "llm = AzureChatOpenAI(model=\"gpt-5-nano\")\n",
    "print(f\"\\n   ü§ñ LLM relevance scoring:\")\n",
    "llm_reranked = llm_relevance_scoring(test_query, documents_only[:4], llm)\n",
    "for i, (doc, score) in enumerate(llm_reranked):\n",
    "    scientist = doc.metadata['scientist_name']\n",
    "    preview = doc.page_content[:30].replace(\"\\n\", \"\\\\n\") + \"...\"\n",
    "    print(f\"      {i + 1}. {scientist} (score: {score:.1f}): {preview}\")\n",
    "    # find original id\n",
    "    for idx, (orig_doc, orig_score) in enumerate(initial_results):\n",
    "        if orig_doc == doc:\n",
    "            original_id = idx\n",
    "            break\n",
    "    print(f\"         (original rank: {original_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a96ac6",
   "metadata": {},
   "source": [
    "Uwaga: Modele jƒôzykowe nie sƒÖ najlepsze to oceny w skali numerycznej i mogƒÖ byƒá kosztowne pod wzglƒôdem zasob√≥w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5b40a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4984ee0",
   "metadata": {},
   "source": [
    "Ensemble rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c362d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_rerank(query, documents, methods=['cross_encoder', 'llm'], weights=None):\n",
    "    \"\"\"Combine multiple re-ranking methods.\"\"\"\n",
    "    if weights is None:\n",
    "        weights = [1.0] * len(methods)\n",
    "\n",
    "    # Normalize weights\n",
    "    total_weight = sum(weights)\n",
    "    weights = [w / total_weight for w in weights]\n",
    "\n",
    "    # Store scores for each method as lists\n",
    "    all_method_scores = []\n",
    "\n",
    "    for method in methods:\n",
    "        if method == 'cross_encoder' and cross_encoders:\n",
    "            model_name = list(cross_encoders.keys())[0]  # Use first available\n",
    "            reranked = cross_encoder_rerank(query, documents, model_name)\n",
    "\n",
    "            # Normalize LLM scores to 0-1 range\n",
    "            max_score = max(score for _, score in reranked) if reranked else 1\n",
    "            min_score = min(score for _, score in reranked) if reranked else -1\n",
    "\n",
    "            normalize = lambda s: (s - min_score) / (max_score - min_score) if max_score > min_score else 0.0\n",
    "\n",
    "            # Create score list in same order as documents\n",
    "            scores = []\n",
    "            reranked_dict = {doc.page_content: normalize(score) for doc, score in reranked}\n",
    "            for doc in documents:\n",
    "                scores.append(reranked_dict.get(doc.page_content, 0))\n",
    "            all_method_scores.append(scores)\n",
    "\n",
    "        elif method == 'llm':\n",
    "            reranked = llm_relevance_scoring(query, documents[:len(documents)], llm)\n",
    "            # Normalize LLM scores to 0-1 range\n",
    "            max_score = max(score for _, score in reranked) if reranked else 10\n",
    "            min_score = min(score for _, score in reranked) if reranked else 0\n",
    "\n",
    "            normalize = lambda s: (s - min_score) / (max_score - min_score) if max_score > min_score else 0.0\n",
    "\n",
    "            scores = []\n",
    "            reranked_dict = {doc.page_content: normalize(score) for doc, score in reranked}\n",
    "            for doc in documents:\n",
    "                scores.append(reranked_dict.get(doc.page_content, 0))\n",
    "            all_method_scores.append(scores)\n",
    "\n",
    "        print(\"Method:\", method)\n",
    "        print(\"Scores:\", all_method_scores[-1])\n",
    "        print()\n",
    "\n",
    "    # Combine scores\n",
    "    final_scores = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        combined_score = 0\n",
    "        for j, method_scores in enumerate(all_method_scores):\n",
    "            if i < len(method_scores):\n",
    "                combined_score += weights[j] * method_scores[i]\n",
    "        final_scores.append((doc, combined_score))\n",
    "\n",
    "    # Sort by combined scores\n",
    "    final_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7779da79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: cross_encoder\n",
      "Scores: [np.float32(1.0), np.float32(0.65593344), np.float32(0.14811605), np.float32(0.039062593), np.float32(0.0)]\n",
      "\n",
      "Method: llm\n",
      "Scores: [1.0, 0.5, 0.5, 0.0, 0.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ensemble_reranked = ensemble_rerank(\n",
    "    test_query,\n",
    "    documents_only[:5],\n",
    "    methods=['cross_encoder', 'llm'],\n",
    "    weights=[0.6, 0.4]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b516028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1. Albert Einstein (ensemble score: 1.000): # Albert Einstein (1879\\n\\n1955)...\n",
      "         (original rank: 0)\n",
      "      2. Albert Einstein (ensemble score: 0.594): Einstein died on April 18, 195...\n",
      "         (original rank: 1)\n",
      "      3. Albert Einstein (ensemble score: 0.289): **Photoelectric Effect\\n\\n**: Ex...\n",
      "         (original rank: 2)\n",
      "      4. Isaac Newton (ensemble score: 0.023): **Universal Gravitation\\n\\n**: N...\n",
      "         (original rank: 3)\n",
      "      5. Albert Einstein (ensemble score: 0.000): Einstein married twice, first ...\n",
      "         (original rank: 4)\n"
     ]
    }
   ],
   "source": [
    "for i, (doc, score) in enumerate(ensemble_reranked):\n",
    "    scientist = doc.metadata['scientist_name']\n",
    "    preview = doc.page_content[:30].replace(\"\\n\", \"\\\\n\") + \"...\"\n",
    "    print(f\"      {i + 1}. {scientist} (ensemble score: {score:.3f}): {preview}\")\n",
    "    # find original id\n",
    "    for idx, (orig_doc, orig_score) in enumerate(initial_results):\n",
    "        if orig_doc == doc:\n",
    "            original_id = idx\n",
    "            break\n",
    "    print(f\"         (original rank: {original_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9257347",
   "metadata": {},
   "source": [
    "Zadanie: Pozosta≈Ça czƒô≈õƒá skryptu `4_reranking.py`:\n",
    "\n",
    "* performance & quality benchmark\n",
    "* re-ranking RAG chain\n",
    "* effectiveness analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef282fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f1398f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "733d61df",
   "metadata": {},
   "source": [
    "RAG evaluation\n",
    "\n",
    "Do oceny RAGa na zajƒôciach u≈ºyjemy biblioteki ragas, natomiast polecam zapoznaƒá siƒô z DeepEval.\n",
    "\n",
    "| Cecha                        | **RAGAS**                                                      | **DeepEval**                                                                       |\n",
    "| ---------------------------- | -------------------------------------------------------------- | ---------------------------------------------------------------------------------- |\n",
    "| **Cel**                      | Ocenianie system√≥w **RAG**                                     | Ocenianie **dowolnych LLM** (RAG, generacja, reasoning, kod, bezpiecze≈Ñstwo, itd.) |\n",
    "| **Zakres metryk**            | Tylko RAG (precision, recall, faithfulness, answer relevancy‚Ä¶) | RAG + generacja tekstu + generacja kodu + testy behawioralne + w≈Çasne metryki      |\n",
    "| **Spos√≥b dzia≈Çania**         | G≈Ç√≥wnie **LLM-as-a-judge** + embeddingi                        | G≈Ç√≥wnie **LLM-as-a-judge**, ale standardowo z testami jednostkowymi (pytest/CI)    |\n",
    "| **Zastosowania**             | Ewaluacja projekt√≥w RAG (akademicko, benchmarki)               | Testy jako≈õci LLM w produkcji (CI/CD)                                              |\n",
    "| **Integracja z narzƒôdziami** | Minimalna (Python)                                             | Bardzo silna: pytest, GitHub Actions, CI/CD                                        |\n",
    "| **Elastyczno≈õƒá**             | Metryki zdefiniowane ‚Äûna sztywno‚Äù                              | Mo≈ºesz budowaƒá **w≈Çasne metryki** i ‚Äûtest cases‚Äù                                   |\n",
    "| **Z≈Ço≈ºono≈õƒá**                | ≈Åatwiejszy start                                               | Trochƒô wiƒôcej konfiguracji                                                         |\n",
    "| **Typowa rola**              | ‚ÄûJak dobrze m√≥j RAG dzia≈Ça?‚Äù                                   | ‚ÄûCzy m√≥j LLM dzia≈Ça poprawnie, stabilnie i bezpiecznie?‚Äù                           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a074f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "778b4d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "def load_and_chunk(data_dir):\n",
    "    loader = DirectoryLoader(data_dir, glob=\"*.txt\")\n",
    "    docs = loader.load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    return splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "def build_rag_system(chunks):\n",
    "    embeddings = AzureOpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    vector_store = InMemoryVectorStore(embeddings)\n",
    "    vector_store.add_documents(documents=chunks)\n",
    "\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 3}\n",
    "    )\n",
    "\n",
    "    llm = AzureChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "    rag_chain = (\n",
    "            {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain, retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498d1629",
   "metadata": {},
   "source": [
    "Tworzymy teraz ground truth - LLM generuje odpowiedzi na podstawie pe≈Çnych dokument√≥w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ground_truths(questions, data_dir, expert_llm):\n",
    "    loader = DirectoryLoader(data_dir, glob=\"*.txt\")\n",
    "    docs = loader.load()\n",
    "    full_context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    ground_truths = []\n",
    "    for q in questions:\n",
    "        prompt = f\"\"\"You are a domain expert with complete knowledge of these scientists.\n",
    "Based on the following complete biographies, provide a comprehensive, accurate answer.\n",
    "\n",
    "Complete Biographies:\n",
    "{full_context}\n",
    "\n",
    "Question: {q}\n",
    "\n",
    "Provide a detailed, factually accurate answer:\"\"\"\n",
    "        ground_truths.append(expert_llm.invoke(prompt).content)\n",
    "    return ground_truths"
   ],
   "id": "b0435590a77796b6"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25b0728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = load_and_chunk(data_dir)\n",
    "rag_chain, retriever = build_rag_system(chunks)\n",
    "expert_llm = AzureChatOpenAI(model=\"gpt-5-nano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445f4022",
   "metadata": {},
   "source": [
    "Uwaga: do ka≈ºdego zadania wykorzystujemy inny model LLM\n",
    "\n",
    "RAG system u≈ºywa modelu \"gpt-4o-mini\", podczas gdy generowanie odpowiedzi eksperckich wykorzystuje \"gpt-5\",\n",
    "natomiast evaluacja u≈ºywa \"gpt-4.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd2cba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What did Marie Curie win Nobel Prizes for?\",\n",
    "    \"What is Einstein's theory of relativity about?\",\n",
    "    \"What are Newton's three laws of motion?\",\n",
    "    \"What did Charles Darwin discover?\",\n",
    "    \"What was Ada Lovelace's contribution to computing?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dff8eea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ground truth answers using expert LLM...\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating ground truth answers using expert LLM...\")\n",
    "ground_truths = generate_ground_truths(questions, data_dir, expert_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "759b4e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate, EvaluationDataset\n",
    "from ragas.metrics import ContextPrecision, ContextRecall, Faithfulness, AnswerRelevancy, FactualCorrectness\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.dataset_schema import SingleTurnSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1229dc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for q, gt in zip(questions, ground_truths):\n",
    "    answer = rag_chain.invoke(q)\n",
    "    contexts = [doc.page_content for doc in retriever.invoke(q)]\n",
    "    samples.append(SingleTurnSample(\n",
    "        user_input=q,  # user question\n",
    "        response=answer,  # generated answer by our RAG system\n",
    "        retrieved_contexts=contexts,  # retrieved contexts by our RAG system\n",
    "        reference=gt  # ground truth answer - generated by expert LLM\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c88ecc",
   "metadata": {},
   "source": [
    "https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/#retrieval-augmented-generation\n",
    "\n",
    "üîµ Context Precision\n",
    "\n",
    "Mierzy, jaka czƒô≈õƒá zwr√≥conych przez retriever kontekst√≥w by≈Ça naprawdƒô potrzebna do odpowiedzi.\n",
    "Im wy≈ºsza precision, tym mniej ‚Äû≈õmieciowych‚Äù chunk√≥w.\n",
    "\n",
    "üü£ Context Recall\n",
    "\n",
    "Mierzy, czy retriever zwr√≥ci≈Ç wszystkie potrzebne konteksty wymagane do odpowiedzi.\n",
    "Niska recall oznacza, ≈ºe kluczowe informacje nie zosta≈Çy odnalezione.\n",
    "\n",
    "üü¢ Faithfulness\n",
    "\n",
    "Sprawdza, czy odpowied≈∫ modelu jest zgodna z przekazanym kontekstem, bez halucynacji.\n",
    "Model nie mo≈ºe dodawaƒá rzeczy spoza retrieved contexts.\n",
    "\n",
    "üü† Answer Relevancy\n",
    "\n",
    "Mierzy, na ile odpowied≈∫ faktycznie odpowiada na pytanie u≈ºytkownika.\n",
    "Ocena: ‚Äûczy odpowied≈∫ jest na temat?‚Äù.\n",
    "\n",
    "üî¥ Factual Correctness\n",
    "\n",
    "Sprawdza, czy odpowied≈∫ jest faktycznie poprawna, por√≥wnujƒÖc jƒÖ z ground truth.\n",
    "Nie chodzi o zgodno≈õƒá z kontekstem, tylko z rzeczywisto≈õciƒÖ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "28099cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_78571/3295105338.py:1: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  evaluator_llm = LangchainLLMWrapper(AzureChatOpenAI(model=\"gpt-4.1\", temperature=0))\n"
     ]
    }
   ],
   "source": [
    "evaluator_llm = LangchainLLMWrapper(AzureChatOpenAI(model=\"gpt-4.1\", temperature=0))\n",
    "embeddings = AzureOpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d42a3bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.57s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_precision': 0.8333}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = SingleTurnSample(\n",
    "    user_input=\"Where is the Eiffel Tower located?\",\n",
    "    retrieved_contexts=[\n",
    "        \"The Eiffel Tower is located in Paris.\",\n",
    "        \"The Brandenburg Gate is located in Berlin.\",\n",
    "        \"The Eiffel Tower is located in France. Paris is the capital city of France.\"\n",
    "    ],\n",
    "    reference=\"The Eiffel Tower is located in Paris.\"\n",
    ")\n",
    "sample_dataset = EvaluationDataset(samples=[sample])\n",
    "results = evaluate(\n",
    "    dataset=sample_dataset,\n",
    "    llm=evaluator_llm,\n",
    "    metrics=[ContextPrecision(llm=evaluator_llm)]\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d68954cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Context Precision: 0.833\n"
     ]
    }
   ],
   "source": [
    "# rƒôcznie:\n",
    "# https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/context_precision/\n",
    "\n",
    "precision_1 = 1 / (1 + 0)\n",
    "v_1 = 1  # poniewa≈º trafili≈õmy na pasujƒÖcƒÖ informacjƒô w pierwszym kontek≈õcie\n",
    "\n",
    "precision_2 = 1 / (1 + 1)\n",
    "v_2 = 0  # poniewa≈º nie trafili≈õmy na pasujƒÖcƒÖ informacjƒô w\n",
    "\n",
    "precision_3 = 2 / (2 + 1)\n",
    "v_3 = 1  # poniewa≈º trafili≈õmy na pasujƒÖcƒÖ informacjƒô w tr\n",
    "\n",
    "context_precision = (v_1 * precision_1 + v_2 * precision_2 + v_3 * precision_3) / (v_1 + v_2 + v_3)\n",
    "print(f\"Calculated Context Precision: {context_precision:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80913b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c442191e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "92b1f09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = EvaluationDataset(samples=samples)\n",
    "\n",
    "metrics = [\n",
    "    ContextPrecision(llm=evaluator_llm),\n",
    "    ContextRecall(llm=evaluator_llm),\n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    AnswerRelevancy(llm=evaluator_llm),\n",
    "    FactualCorrectness(llm=evaluator_llm)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "69835fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/25 [00:00<?, ?it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   4%|‚ñç         | 1/25 [00:05<02:01,  5.08s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  32%|‚ñà‚ñà‚ñà‚ñè      | 8/25 [00:48<01:22,  4.88s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [01:46<00:00,  4.28s/it]\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(dataset=eval_dataset, metrics=metrics, llm=evaluator_llm, embeddings=embeddings)\n",
    "df = result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What did Marie Curie win Nobel Prizes for?</td>\n",
       "      <td>[Marie Sklodowska - Curie (1867-1934) Marie Sk...</td>\n",
       "      <td>Marie Curie won the Nobel Prize in Physics in ...</td>\n",
       "      <td>Marie Curie won two Nobel Prizes:\\n\\n- 1903 No...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.879175</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is Einstein's theory of relativity about?</td>\n",
       "      <td>[# Albert Einstein (1879\\n\\n1955)\\n\\nAlbert Ei...</td>\n",
       "      <td>Einstein's theory of relativity comprises two ...</td>\n",
       "      <td>Einstein‚Äôs theory of relativity (often simply ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.725321</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are Newton's three laws of motion?</td>\n",
       "      <td>[Newton attended The King's School in Grantham...</td>\n",
       "      <td>Newton's three laws of motion are: 1) An objec...</td>\n",
       "      <td>Isaac Newton‚Äôs three laws of motion are:\\n\\n1)...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What did Charles Darwin discover?</td>\n",
       "      <td>[# Charles Darwin (1809\\n\\n1882)\\n\\nCharles Ro...</td>\n",
       "      <td>Charles Darwin discovered the theory of evolut...</td>\n",
       "      <td>Charles Darwin‚Äôs flagship discovery was the th...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.774263</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What was Ada Lovelace's contribution to comput...</td>\n",
       "      <td>[Throughout her adult life, Lovelace struggled...</td>\n",
       "      <td>Ada Lovelace is known for her work on Charles ...</td>\n",
       "      <td>Ada Lovelace‚Äôs contribution to computing cente...</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.861590</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0         What did Marie Curie win Nobel Prizes for?   \n",
       "1     What is Einstein's theory of relativity about?   \n",
       "2            What are Newton's three laws of motion?   \n",
       "3                  What did Charles Darwin discover?   \n",
       "4  What was Ada Lovelace's contribution to comput...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [Marie Sklodowska - Curie (1867-1934) Marie Sk...   \n",
       "1  [# Albert Einstein (1879\\n\\n1955)\\n\\nAlbert Ei...   \n",
       "2  [Newton attended The King's School in Grantham...   \n",
       "3  [# Charles Darwin (1809\\n\\n1882)\\n\\nCharles Ro...   \n",
       "4  [Throughout her adult life, Lovelace struggled...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Marie Curie won the Nobel Prize in Physics in ...   \n",
       "1  Einstein's theory of relativity comprises two ...   \n",
       "2  Newton's three laws of motion are: 1) An objec...   \n",
       "3  Charles Darwin discovered the theory of evolut...   \n",
       "4  Ada Lovelace is known for her work on Charles ...   \n",
       "\n",
       "                                           reference  context_precision  \\\n",
       "0  Marie Curie won two Nobel Prizes:\\n\\n- 1903 No...           0.500000   \n",
       "1  Einstein‚Äôs theory of relativity (often simply ...           1.000000   \n",
       "2  Isaac Newton‚Äôs three laws of motion are:\\n\\n1)...           0.000000   \n",
       "3  Charles Darwin‚Äôs flagship discovery was the th...           1.000000   \n",
       "4  Ada Lovelace‚Äôs contribution to computing cente...           0.583333   \n",
       "\n",
       "   context_recall  faithfulness  answer_relevancy  \\\n",
       "0        0.857143      1.000000          0.879175   \n",
       "1        0.277778      0.888889          0.725321   \n",
       "2        0.000000      0.285714          1.000000   \n",
       "3        0.909091      0.800000          0.774263   \n",
       "4        0.800000      1.000000          0.861590   \n",
       "\n",
       "   factual_correctness(mode=f1)  \n",
       "0                          0.78  \n",
       "1                          0.26  \n",
       "2                          0.26  \n",
       "3                          0.38  \n",
       "4                          0.64  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "id": "25e5c4f1a7f42941"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c1c7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65b4bc37",
   "metadata": {},
   "source": [
    "Zadanie domowe: w module multi_rag_evaluation znajduje siƒô kod do oceny systemu RAG z wykorzystaniem r√≥≈ºnych metryk i r√≥≈ºnych podej≈õƒá RAGowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6fb96a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
