# Module 3: Document Loading for RAG Systems

This module teaches how to ingest content from different source formats â€”
plain text files, PDFs, and live web pages â€” and feed it into a
Retrieval-Augmented Generation pipeline using **LangChain** and **Azure OpenAI**.

## ğŸ¯ Learning Objectives

By completing this module, you will:

- Master text, PDF, and web-based document loading strategies
- Understand trade-offs between loading approaches (single file, directory, glob patterns)
- Convert text files to PDF and extract content back with PyPDFLoader
- Scrape and clean web pages with WebBaseLoader and BeautifulSoup
- Split documents into overlapping chunks for vector search
- Build complete RAG chains over each source type
- Apply best practices for production document ingestion

## ğŸ“š Module Content

### 1. Text File Loading (`1_text_files.py`)

**ğŸ“„ Four strategies for loading text into a RAG pipeline**

A step-by-step script covering:

- **Single File Loading** â€” `TextLoader` for one document at a time
- **Multiple Specific Files** â€” iterating over a curated list of paths
- **Directory Loading** â€” `DirectoryLoader` for automatic file discovery
- **Glob Pattern Filtering** â€” selective discovery with `*.txt` patterns
- **Text Splitting** â€” `RecursiveCharacterTextSplitter` with chunk distribution analysis
- **RAG Chain** â€” end-to-end question-answering over scientist biographies

Key comparison of loading methods:

| Method                  | Advantages                       | Disadvantages                  |
|-------------------------|----------------------------------|--------------------------------|
| Single File             | Precise control, fast            | Manual, doesn't scale          |
| Multiple Specific Files | Selective, curated               | Requires file list maintenance |
| Directory Loading       | Automatic discovery, scales well | May include unwanted files     |
| Pattern-Based Loading   | Flexible filtering, best of both | â€”                              |

### 2. PDF Document Processing (`2_pdf_loading.py`)

**ğŸ“„ Full PDF lifecycle â€” creation, extraction, comparison, and RAG**

Covers the complete workflow:

- **PDF Creation** â€” converting text biographies to styled PDFs with ReportLab
- **Page-Level Loading** â€” `PyPDFLoader` produces one `Document` per page with metadata
- **PDF vs Text Comparison** â€” character count differences and formatting artifact analysis
- **Chunk Distribution** â€” splitting PDF pages with overlap for vector search
- **RAG Chain** â€” question-answering with a prompt aware of PDF artifacts
- **Best Practices** â€” when to choose PDF loading, its advantages and pitfalls

### 3. Web Content Integration (`3_web_sources.py`)

**ğŸŒ Live web scraping, HTML cleanup, and web-based RAG**

End-to-end web content ingestion:

- **Source Configuration** â€” Simple English Wikipedia URLs for clean demo content
- **URL Accessibility Checks** â€” HEAD/GET probes before loading
- **WebBaseLoader** â€” fetching pages with browser-like User-Agent headers
- **BeautifulSoup Cleaning** â€” removing nav/script/footer elements, collapsing whitespace
- **Structured Extraction** â€” title, section headings, and body from raw HTML
- **Web-Tuned Chunking** â€” separator hierarchy tailored to HTML-extracted prose
- **RAG Chain** â€” question-answering with web-sourced context
- **Best Practices** â€” rate limiting, legal/ethical considerations, error handling

## ğŸ“Š Document Type Comparison

| Feature               | Text Files     | PDF Files             | Web Sources          |
|-----------------------|----------------|-----------------------|----------------------|
| **Processing Speed**  | ğŸŸ¢ Fastest     | ğŸŸ¡ Medium             | ğŸ”´ Slowest           |
| **Content Quality**   | ğŸŸ¢ Clean       | ğŸŸ¡ May have artifacts | ğŸŸ¡ Needs cleaning    |
| **Reliability**       | ğŸŸ¢ High        | ğŸŸ¢ High               | ğŸŸ¡ Network dependent |
| **Setup Complexity**  | ğŸŸ¢ Simple      | ğŸŸ¡ Medium             | ğŸŸ¡ Medium            |
| **Metadata Richness** | ğŸ”´ Basic       | ğŸŸ¢ Rich (pages)       | ğŸŸ¡ Medium (headings) |
| **Best For**          | Simple content | Formal documents      | Live / current data  |

## ğŸš€ Quick Start

### Prerequisites

- Python 3.13+
- Azure OpenAI credentials in `.env`
- Internet access (for `3_web_sources.py`)

### Running the Scripts

```bash
# Text file loading â€” four strategies + RAG chain
uv run python "03_document_loading/1_text_files.py"

# PDF processing â€” creates sample PDFs, then loads and queries them
uv run python "03_document_loading/2_pdf_loading.py"

# Web content â€” scrapes Wikipedia, cleans HTML, runs RAG (requires internet)
uv run python "03_document_loading/3_web_sources.py"
```

## âœ… Expected Behaviour

**`1_text_files.py`**

- Demonstrates four loading approaches with scientist biography files
- Prints chunk distribution per scientist after splitting
- Answers three test questions via RAG chain
- Displays a comparison table of loading methods

**`2_pdf_loading.py`**

- Creates `data/pdfs/` with styled PDFs (Ada Lovelace, Albert Einstein)
- Shows page-level metadata from PyPDFLoader
- Compares character counts between PDF and plain-text representations
- Answers two test questions via PDF-based RAG chain
- Prints best-practices summary

**`3_web_sources.py`**

- Lists configured Wikipedia sources and probes accessibility
- Loads pages with WebBaseLoader, then cleans with BeautifulSoup
- Extracts structured metadata (title, headings) from HTML
- Creates web-tuned chunks and answers three test questions via RAG
- Prints best-practices and legal/ethical summary

## ğŸ› ï¸ Key Dependencies

| Package            | Purpose                                |
|--------------------|----------------------------------------|
| `langchain`        | Document loaders, text splitters       |
| `langchain-openai` | AzureChatOpenAI, AzureOpenAIEmbeddings |
| `pypdf`            | PDF text extraction (PyPDFLoader)      |
| `reportlab`        | PDF creation for demonstrations        |
| `requests`         | HTTP requests for URL probing          |
| `beautifulsoup4`   | HTML parsing and content cleaning      |
| `python-dotenv`    | `.env` file loading                    |

## âš ï¸ Common Issues

- **PDF extraction quality** â€” some PDFs yield poor text; test with your own files
- **Network failures** â€” `3_web_sources.py` raises `RuntimeError` if no URLs are accessible
- **Rate limiting** â€” web script pauses 1 s between requests to respect server limits
- **File encoding** â€” all text files must be UTF-8 compatible
- **Write permissions** â€” `2_pdf_loading.py` creates the `data/pdfs/` directory

## ğŸ“ Content Processing Pipeline

1. **Source Detection** â€” identify file type and choose the appropriate loader
2. **Content Extraction** â€” format-specific extraction (text / PDF pages / HTML)
3. **Cleaning** â€” remove artifacts, collapse whitespace, strip navigation elements
4. **Metadata Enrichment** â€” add source tracking, page numbers, section headings
5. **Chunking** â€” split with `RecursiveCharacterTextSplitter` (overlap for context)
6. **Indexing & RAG** â€” embed chunks, store in vector store, query with LLM

## ğŸš€ Next Steps

After mastering document loading, continue with:

- **04 Advanced Retrieval** â€” multi-query retrieval, re-ranking, hybrid search
- **05 RAG Evaluation** â€” measuring retrieval and generation quality
- **06 GraphRAG** â€” combining knowledge graphs with retrieval
